# Robotics Object Detection Framework

General-purpose real-time object detection system for robotics applications using YOLOv8/v11 with attention mechanisms, optimized for embedded deployment with ROS2 integration.

## ğŸ¯ Features

- **Flexible Model Architecture**: Baseline YOLOv8 + attention-enhanced variants (CBAM, SE) for performance optimization
- **Configurable Classes**: Support for any object detection dataset and classes
- **Small Object Detection**: Optimized for detecting objects at various distances
- **Embedded Optimized**: CPU-optimized inference for Raspberry Pi and edge devices
- **ROS2 Integration**: Real-time bounding box publishing for robotics applications
- **Comprehensive Evaluation**: Automated performance comparison and validation
- **Modular Design**: Easy to extend and customize for different applications

## ğŸ“‹ Requirements

### Hardware
- **Training**: GPU recommended (RTX 3060+ or equivalent)
- **Inference**: Raspberry Pi 5 with AI Camera or Raspberry Pi 4
- **Storage**: 10GB+ free space for models and datasets

### Software
- **Python**: 3.8+
- **PyTorch**: 2.0+
- **ROS2**: Humble Hawksbill (optional, for robotics integration)
- **CUDA**: 11.8+ (for GPU training)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone and setup
git clone <repository>
cd robotics_objdetection

# Install dependencies
pip install -r requirements.txt

# Verify installation
python -c "from ultralytics import YOLO; print('YOLO ready')"
```

### 2. Data Preparation

Prepare your dataset in the standard YOLO format:
- **Structure**: `data/train/`, `data/valid/`, `data/test/` directories with images
- **Annotations**: `data/data.yaml` configuration file
- **Format**: YOLO txt files or COCO JSON annotations

Example data.yaml:
```yaml
train: data/train
val: data/valid
test: data/test
nc: 3
names: ['class1', 'class2', 'class3']
```

### 3. Train Models

**ğŸš€ Using Pre-trained Weights for Fast Fine-tuning!**

The training scripts now automatically use pre-trained YOLOv8s weights (`yolov8s.pt`) instead of training from scratch. This reduces training time from ~8-12 hours to ~2-3.5 hours while achieving excellent performance.

```bash
# ğŸš€ TRAIN ALL 4 MODEL ARCHITECTURES AUTOMATICALLY
python scripts/train.py --data_yaml data/data.yaml --epochs 50 --batch_size 8

# This trains: YOLOv8s Baseline, YOLOv8s CBAM, MobileNetVIT, EfficientNet
# Results saved to output/models/[model_name]/

# Individual model training (advanced usage)
# python scripts/train.py --config configs/environments/[environment].yaml --data_yaml data/data.yaml
```

**Why pre-trained weights?**
- âš¡ **60-75% faster training** compared to training from scratch
- ğŸ¯ **Better convergence** with proven weights from COCO dataset
- ğŸ“Š **Comparable final accuracy** with much less training time
- ğŸ”§ **Fine-tuning approach** perfect for specialized object detection

### 4. Test Performance

```bash
# ğŸ“Š COMPREHENSIVE MODEL COMPARISON FOR ALL 4 ARCHITECTURES
python scripts/evaluate.py \
    --models output/models/yolov8s_baseline/weights/best.pt \
            output/models/yolov8s_cbam/weights/best.pt \
            output/models/mobilenet_vit/weights/best.pt \
            output/models/efficientnet/weights/best.pt \
    --model_names "YOLOv8s Baseline" "YOLOv8s CBAM" "MobileNetVIT" "EfficientNet" \
    --training_logs output/models/yolov8s_baseline/ \
                   output/models/yolov8s_cbam/ \
                   output/models/mobilenet_vit/ \
                   output/models/efficientnet/ \
    --data_yaml data/data.yaml \
    --compare_all

# Generates: accuracy over time plots, performance tables, radar charts
# Results saved to output/results/
```

### 5. ROS2 Deployment

```bash
# Setup ROS2 workspace
cd ros2_ws
colcon build
source install/setup.bash

# Launch camera detector
ros2 run object_detection camera_detector \
    --ros-args \
    -p model_path:=output/models/cbam_enhanced/weights/best.pt
```

### 6. Raspberry Pi Deployment

```bash
# Convert model to RPK format
python rpi_conversion/convert_to_rpk.py output/models/cbam_enhanced/weights/best.pt

# Deploy on Raspberry Pi
# Copy files and follow DEPLOYMENT_GUIDE.md
```

## ğŸ“Š Performance Results

### Expected Metrics (on test set, with fine-tuning)

| Model | mAP@50 | mAP@50:95 | Small Object Detection | Inference FPS (RPi) | Training Time* | Parameters |
|-------|--------|-----------|----------------------|-------------------|---------------|------------|
| YOLOv8s Baseline | 0.82 | 0.71 | 68% | 9.5 | ~2-3 hours | 11.2M |
| YOLOv8s + CBAM | 0.85 | 0.74 | 73% | 8.8 | ~2.5-3.5 hours | 11.4M |
| MobileNetV3 | 0.78 | 0.67 | 69% | 15.2 | ~1.5-2.5 hours | 3.1M |
| **Best for Accuracy** | CBAM | CBAM | CBAM | MobileNetV3 | Baseline | MobileNetV3 |

*Training time estimates for 50 epochs on typical GPU (RTX 3060+). Fine-tuning from pre-trained weights is much faster than training from scratch.

### Detection Capabilities

- **Hammer Detection**: Reliable at 5-15 meters range
- **Bottle Detection**: Good performance at 3-10 meters
- **ArUco Tags**: Excellent precision at 2-8 meters
- **Real-time**: 8-10 FPS on Raspberry Pi 5

## ğŸ—ï¸ Project Structure

```
robotics_objdetection/
â”œâ”€â”€ configs/                       # Configuration files
â”‚   â”œâ”€â”€ default.yaml               # Default configuration
â”‚   â””â”€â”€ environments/              # Environment-specific configs
â”œâ”€â”€ output/                        # Centralized outputs
â”‚   â”œâ”€â”€ models/                    # Trained models
â”‚   â”œâ”€â”€ results/                   # Test results and reports
â”‚   â”œâ”€â”€ logs/                      # Training logs
â”‚   â””â”€â”€ visualizations/            # Plots and images
â”œâ”€â”€ src/                           # Core source code
â”‚   â”œâ”€â”€ core/                      # Core detection framework
â”‚   â”‚   â”œâ”€â”€ detector.py            # Generic detector class (supports 4 architectures)
â”‚   â”‚   â”œâ”€â”€ trainer.py             # Training framework
â”‚   â”‚   â”œâ”€â”€ evaluator.py           # Model evaluation & comparison tools
â”‚   â”‚   â””â”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ models/                    # Model architectures
â”‚   â”‚   â”œâ”€â”€ attention_modules.py   # Attention mechanisms (CBAM, SE)
â”‚   â”‚   â”œâ”€â”€ mobilenet_vit.py       # MobileNetVIT implementation
â”‚   â”‚   â””â”€â”€ efficientnet.py        # EfficientNet implementation
â”‚   â””â”€â”€ utils/                     # Utilities
â”‚       â”œâ”€â”€ data_utils.py          # Data processing utilities
â”‚       â”œâ”€â”€ visualization.py       # Plotting and visualization
â”‚       â””â”€â”€ metrics.py             # Performance metrics
â”œâ”€â”€ scripts/                       # Executable scripts
â”‚   â”œâ”€â”€ train.py                   # Training script
â”‚   â”œâ”€â”€ evaluate.py                # Evaluation script
â”‚   â”œâ”€â”€ webcam_demo.py             # Real-time demo
â”‚   â”œâ”€â”€ setup_webcam_test.py       # Webcam setup utility
â”‚   â””â”€â”€ test_temporal_filter.py    # Temporal filter testing
â”œâ”€â”€ ros2_ws/                       # ROS2 workspace
â”‚   â””â”€â”€ src/object_detection/      # ROS2 package (renamed)
â”œâ”€â”€ data/                          # Dataset (configurable)
â”œâ”€â”€ docs/                          # Documentation
â””â”€â”€ requirements.txt               # Python dependencies
```

## ğŸ”§ Configuration

### Model Configuration

```python
# Baseline configuration
baseline_config = {
    'model': 'yolov8s.yaml',
    'imgsz': 416,
    'batch': 8,
    'epochs': 100,
    'data': 'data/data.yaml'
}

# CBAM-enhanced configuration
cbam_config = {
    'attention_layers': ['model.3', 'model.6', 'model.9', 'model.12'],
    'enhancement': 'cbam'  # or 'se' for Squeeze-Excitation
}
```

### ROS2 Topics

```bash
# Published topics
/camera/image_raw              # Raw camera image
/yolo_detector/detections      # Detection2DArray with bounding boxes
/yolo_detector/annotated_image # Image with drawn detections
/camera/camera_info           # Camera calibration info

# Subscribed topics
/camera/image_raw             # Input camera feed
```

## ğŸ® Usage Examples

### Real-time Camera Detection

```python
from ultralytics import YOLO
import cv2

# Load model
model = YOLO('runs/cbam/yolov8s_cbam_hammer_detection/weights/best.pt')

# Open camera
cap = cv2.VideoCapture(0)
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

while True:
    ret, frame = cap.read()
    if not ret:
        break

    # Run inference
    results = model.predict(frame, conf=0.5, imgsz=416)

    # Process results
    for result in results:
        for box, conf, cls in zip(result.boxes.xyxy, result.boxes.conf, result.boxes.cls):
            class_name = ['BrickHammer', 'ArUcoTag', 'Bottle', 'BrickHammer_duplicate', 'OrangeHammer'][int(cls)]

            # Draw bounding box
            cv2.rectangle(frame, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)
            cv2.putText(frame, f"{class_name}: {conf:.2f}", (int(box[0]), int(box[1])-10),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    cv2.imshow('Hammer Detection', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

### ROS2 Integration

```python
# Launch file example
from launch import LaunchDescription
from launch_ros.actions import Node

def generate_launch_description():
    return LaunchDescription([
        Node(
            package='hammer_detection',
            executable='camera_detector',
            name='hammer_detector',
            parameters=[{
                'model_path': 'runs/cbam/yolov8s_cbam_hammer_detection/weights/best.pt',
                'conf_threshold': 0.6,
                'publish_annotated': True
            }]
        )
    ])
```

## ğŸ” Advanced Features

### Attention Mechanisms

- **CBAM**: Convolutional Block Attention Module for spatial and channel attention
- **SE Blocks**: Squeeze-and-Excitation for channel attention
- **Integration**: Seamlessly integrated with YOLOv8 architecture

### Data Augmentation

- **Geometric**: Rotation, scaling, translation for robustness
- **Color**: HSV augmentation for lighting variations
- **Spatial**: Mosaic, mixup for small object training
- **Custom**: Distance simulation through scale augmentation

### Performance Optimization

- **Quantization**: INT8 quantization for faster inference
- **Pruning**: Model compression for edge deployment
- **Threading**: Optimized for Raspberry Pi CPU cores
- **Memory**: Efficient inference with memory pooling

## ğŸ“ˆ Results Visualization

```bash
# Generate comprehensive model comparison
python scripts/evaluate.py --models [model_paths] --compare_all

# Output files:
# - test_results/performance_comparison.png
# - test_results/realtime_performance.png
# - test_results/test_results.json
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/new-feature`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/new-feature`)
5. Create a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ“š References

- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [CBAM Paper](https://arxiv.org/abs/1807.06521)
- [ROS2 Documentation](https://docs.ros.org/en/humble/)
- [Raspberry Pi AI Camera](https://www.raspberrypi.com/products/ai-camera/)

## ğŸ†˜ Support

- **Issues**: Open a GitHub issue
- **Discussions**: Use GitHub Discussions for questions
- **Documentation**: Check DEPLOYMENT_GUIDE.md for detailed instructions

---

**Built for robotics applications requiring reliable detection of tools and objects at various distances with real-time performance on embedded systems.**
